# v0.6 Phase 5: Pilot Quality Validation & Output Elevation

## 1. Phase Definition

*   **Phase Name:** v0.6 Phase 5 â€” Pilot Quality Validation & Output Elevation
*   **Core Problem:** Infrastructure is ready, but content output risks being "generic AI slop." We need to prove the system can produce specific, highly relevant, expert-grade training material that relies on messy real-world context, not just generic knowledge.
*   **Exit Criteria:**
    1.  **One Single Pilot Course** successfully generated, governed, and packaged.
    2.  **Quality Verified:** SME rating > 4/5 on "Nuance & Realism" (vs baseline 2/5).
    3.  **Governance Intact:** 100% of Pass 2 assets trace back to Pass 1 requirements.
    4.  **Executive Package:** A "Boardroom Ready" evidence bundle showing Before/After comparison.

---

## 2. Quality Elevation Mechanisms (Concrete, Minimal)

We will introduce three surgical mechanisms to force specificity and eliminate genericism.

### Mechanism A: The "Scenario Anchor" Injection
*   **Where:** Pass 2 Input (Production Factory).
*   **Code Surface:** `inputs/sme_notes.md` (Template update) + `agents/instructional_designer/prompts/system.md`.
*   **Mechanism:**
    *   SMEs must provide a **"Central Case Study"** (a specific, messy, real-world failure or scenario).
    *   The ID Agent is strictly prompted: *"Every concept must be explained through the lens of the Central Case Study. Do not explain the concept in abstract."*
*   **Validation:** Human Review (Pass 2 Gate) - "Does this script mention the case study names/details?"

### Mechanism B: The "Anti-Generic" Linter
*   **Where:** Pass 2 Output Validator (CI / Pipeline).
*   **Code Surface:** `orchestrator/quality_checks.py` (New lightweight module).
*   **Mechanism:**
    *   A regex/heuristic scanner that flags "Banned Phrases" typical of low-effort AI.
    *   *Examples:* "In the dynamic world of...", "It is important to remember...", "landscape", "tapestry", "game-changer".
*   **Governance Check:** If > 3 banned phrases found per script, `run_pipeline.py` marks the step as **WARNING** (Soft Block).
*   **Evidence:** "Quality Log" showing reduction in drift words.

### Mechanism C: Role-Immersive System Prompts
*   **Where:** Pass 2 Agent Configuration.
*   **Code Surface:** `orchestrator/agents.py` (Runtime injection).
*   **Mechanism:**
    *   Instead of "You are an Instructional Designer", we inject the specific *SME Role* from `course_architecture.json` into the ID's identity.
    *   *Prompt Change:* "You are not an AI. You are [Role defined in Architecture]. You use the vocabulary, slang, and prioritization of a [Role]."
*   **Evidence:** Vocabulary analysis (presence of domain-specific terms vs generic terms).

---

## 3. Single-Course Pilot Design

### Execution Model
*   **Topic Selection:** Must be **High-Context / Operational**.
    *   *Good:* "Troubleshooting the XY-900 Hydraulic Pump Seal."
    *   *Bad:* "Leadership Skills for Managers." (Too subjective, easy to fake).
*   **SME Model:** 1 Hour "Input Dump" + 1 Hour "Critique". (Total 2h SME time).

### Success Metrics (Scoring Model)
We will score the Pilot Course on a 0-5 scale.

| Metric | Definition | Success Threshold |
| :--- | :--- | :--- |
| **Ploppability** | Can I deploy this zip file to the LMS *today* without editing? | 100% (Binary) |
| **Specificity** | % of paragraphs appearing to be written for *this specific company* vs any company. | > 80% |
| **New Hire Safe** | Would I feel safe sending this to a Day 1 employee? (Likert 1-5) | 4+ |
| **Hallucination** | Number of factually incorrect procedural steps. | 0 |

### Pilot Governance
1.  **Input Gate:** Architect reviews `sme_notes` for "Case Study" presence.
2.  **Structural Gate (Pass 1):** Architect confirms Learning Objects match valid taxonomy.
3.  **Media Gate (Pass 2):** SME reviews 3 Slide Decks + Scripts. "Go/No-Go" for specific slides.

---

## 4. Minimal Engineering Changes

No refactors. Only accretive updates.

### A. Operations & Integration
1.  **Update Input Templates:**
    *   Modify `inputs/sme_notes.md` to include a required `## Central Case Study` section.
2.  **Linting Logic:**
    *   Add `scripts/check_quality_signals.py` (Stand-alone analysis script).
    *   Add `banned_phrases.txt` config file.

### B. Agent Prompts
3.  **Instructional Designer Update:**
    *   Inject Instruction: "ANCHOR_TO_CASE_STUDY: [True/False]".
    *   Inject Config: "BANNED_ISMS_LIST".

### C. Reporting
4.  **Pilot Scorecard:**
    *   Update `src/reporting` to generate a `pilot_scorecard_template.md` at the end of the run for the human to fill in.

---

## 5. Evidence Package for Leadership

The final output of Phase 5 is the **"Pilot Defense Bundle"**.

1.  **The Artifact:** The actual `course_bundle.zip` (SCORM/HTML5).
2.  **The "Drift Report":**
    *   Left Column: "Pass 1 Requirement (LO Description)"
    *   Right Column: "Pass 2 Execution (Actual Script Snippet)"
    *   *Proof of Governance.*
3.  **The "Before/After" snippet:**
    *   *Standard AI:* "In this module, we will explore pump safety..."
    *   *Phase 5 AI:* "Look at the seal on the XY-900. If it's cracked, stop."
4.  **The Score:** The finalized `pilot_scorecard.md` signed by the SME.

**Recommendation Stance:**
*   **GO:** Score > 4.0 avg, 0 Critical Hallucinations.
*   **NO-GO:** Score < 3.5, or any "Safety Critical" failure.

---

## 6. Hard Stop Boundary (Out of Scope)

*   **NO** Automatic iterative refinement (Agent criticizing itself).
*   **NO** Multi-stakeholder voting UI.
*   **NO** Video generation (Text/Image/Voice only).
*   **NO** Direct API integration with LMS (Zip file handoff only).
*   **Triggers for Enterprise Scale (v0.7):**
    *   Pilot successful.
    *   Leadership approves budget for multi-tenant infrastructure based on Cost/Quality evidence.
